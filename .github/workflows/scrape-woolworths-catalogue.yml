name: Woolworths Catalogue

on:
  schedule:
    - cron: '0 20 * * 0'   # Sunday 6am AEST (UTC+10)
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r scraper/requirements.txt
          playwright install chromium --with-deps

      - name: Scrape Woolworths catalogue
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          WOOLWORTHS_HEADLESS: "true"
        run: xvfb-run --auto-servernum python -m scraper.main catalogue woolworths

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: woolworths-catalogue-logs-${{ github.run_id }}
          path: scraper/logs/
          retention-days: 14

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸ”´ Woolworths Catalogue scrape failed â€” ${new Date().toISOString().split('T')[0]}`;
            const body = [
              `**Workflow:** ${context.workflow}`,
              `**Run:** [#${context.runNumber}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              `**Triggered by:** ${context.eventName}`,
              ``,
              `Check the run logs and uploaded artifacts for details.`,
            ].join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['scraper-failure'],
            });
